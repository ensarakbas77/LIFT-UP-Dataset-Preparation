import fitz
import re
import csv
import sys
import os
import glob

# ====================================================================
# CONFIGURATION - Buradan PDF yolunu ve ayarlarÄ± deÄŸiÅŸtirebilirsiniz
# ====================================================================

# YÄ±l bilgisi (CSV'ye yazÄ±lacak)
YEAR = "2021-2022"

# PDF dosyasÄ±, klasÃ¶rÃ¼ veya glob pattern
PDF_PATH = f"Bildiri-Kitabi-{YEAR}.pdf"

# Ã‡Ä±ktÄ± klasÃ¶rÃ¼ (None ise PDF ile aynÄ± yerde oluÅŸturulur)
OUTPUT_DIR = None


# ====================================================================
# TEXT UTILITIES - Metin iÅŸleme yardÄ±mcÄ± fonksiyonlarÄ±
# ====================================================================

TR_CHARS = set("ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅÄ°Ã–Ã‡")


def clean_text(text: str) -> str:
    """
    Metindeki fazla boÅŸluklarÄ± temizler ve strip yapar.
    
    Args:
        text: Temizlenecek metin
        
    Returns:
        TemizlenmiÅŸ metin
    """
    if not text:
        return ""
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def contains_tr_char(s: str) -> bool:
    """
    Metinde TÃ¼rkÃ§e karakter olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    
    Args:
        s: Kontrol edilecek metin
        
    Returns:
        True/False
    """
    return any(c in TR_CHARS for c in s)


def looks_english_line(s: str) -> bool:
    """
    SatÄ±rÄ±n Ä°ngilizce baÅŸlÄ±k satÄ±rÄ± olup olmadÄ±ÄŸÄ±nÄ± heuristik yÃ¶ntemle kontrol eder.
    TR karakteri iÃ§ermeyen ve yaygÄ±n Ä°ngilizce teknik terimleri iÃ§eren satÄ±rlarÄ± yakalar.
    
    Args:
        s: Kontrol edilecek satÄ±r
        
    Returns:
        True ise muhtemelen Ä°ngilizce satÄ±r
    """
    if not s:
        return False
    if contains_tr_char(s):
        return False
    
    low = s.lower()
    english_hints = [
        "production", "testing", "used in", "using", "technology",
        "system", "systems", "analysis", "design", "optimization",
        "manufacturing", "additive"
    ]
    return any(hint in low for hint in english_hints)


# ====================================================================
# CONTENT DETECTION - Ä°Ã§erik tespiti fonksiyonlarÄ±
# ====================================================================

def is_article_start_page(text: str) -> bool:
    """
    SayfanÄ±n yeni bir makale baÅŸlangÄ±cÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    Hem "Ã–zetÃ§e" hem "Abstract" kelimelerini iÃ§eriyorsa makale baÅŸlangÄ±cÄ±dÄ±r.
    
    Args:
        text: Sayfa metni
        
    Returns:
        True ise yeni makale baÅŸlangÄ±cÄ±
    """
    return ("Ã–zetÃ§e" in text) and ("Abstract" in text)


def collect_until_markers(doc, start_idx: int, stop_markers: list, hard_limit: int = 8) -> str:
    """
    Belirli bir sayfadan baÅŸlayarak, durma iÅŸaretÃ§ilerine kadar olan metni toplar.
    Ã–zet Ã§Ä±karÄ±mÄ±nda kullanÄ±lÄ±r (bir Ã¶zet birkaÃ§ sayfaya yayÄ±labilir).
    
    Args:
        doc: PDF dÃ¶kÃ¼manÄ±
        start_idx: BaÅŸlangÄ±Ã§ sayfa indeksi
        stop_markers: Durma iÅŸaretÃ§ileri listesi (Ã¶rn: ["Keywords", "Anahtar Kelimeler"])
        hard_limit: Maksimum kaÃ§ sayfa toplanacak (varsayÄ±lan: 8)
        
    Returns:
        BirleÅŸtirilmiÅŸ metin
    """
    parts = []
    for i in range(start_idx, min(len(doc), start_idx + hard_limit)):
        page_text = doc[i].get_text()
        
        # Yeni makale baÅŸladÄ±ysa dur
        if i > start_idx and is_article_start_page(page_text):
            break
            
        parts.append(page_text)
        
        # Durma iÅŸaretÃ§ilerini kontrol et
        low = page_text.lower()
        if any(marker.lower() in low for marker in stop_markers):
            break
            
    return "\n".join(parts)


# ====================================================================
# ABSTRACT EXTRACTION - Ã–zet Ã§Ä±karma fonksiyonlarÄ±
# ====================================================================

def extract_abstract_tr(text: str) -> str:
    """
    TÃ¼rkÃ§e Ã¶zeti "Ã–zetÃ§eâ€”" ile "Anahtar Kelimeler" arasÄ±ndan Ã§Ä±karÄ±r.
    
    Args:
        text: Ä°ÅŸlenecek metin
        
    Returns:
        TÃ¼rkÃ§e Ã¶zet
    """
    pattern = r"Ã–zetÃ§e\s*[â€”\-â€“]+\s*(.*?)\s*(?=Anahtar\s*Kelimeler)"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    return clean_text(match.group(1)) if match else ""


def extract_abstract_en(text: str) -> str:
    """
    Ä°ngilizce Ã¶zeti "Abstractâ€”" ile "Keywords" arasÄ±ndan Ã§Ä±karÄ±r.
    
    Args:
        text: Ä°ÅŸlenecek metin
        
    Returns:
        Ä°ngilizce Ã¶zet
    """
    pattern = r"Abstract\s*[â€”\-â€“]+\s*(.*?)\s*(?=Keywords)"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    return clean_text(match.group(1)) if match else ""


def extract_keywords_tr(text: str) -> str:
    """
    TÃ¼rkÃ§e anahtar kelimeleri "Anahtar Kelimelerâ€”" ile "Abstract" arasÄ±ndan Ã§Ä±karÄ±r.
    
    Anahtar kelimeler genellikle TÃ¼rkÃ§e Ã¶zetin hemen ardÄ±ndan gelir ve virgÃ¼lle ayrÄ±lmÄ±ÅŸ
    kelime/kelime gruplarÄ± ÅŸeklindedir.
    
    Args:
        text: Ä°ÅŸlenecek metin
        
    Returns:
        TÃ¼rkÃ§e anahtar kelimeler (virgÃ¼lle ayrÄ±lmÄ±ÅŸ)
    """
    # "Anahtar Kelimeler" ile "Abstract" arasÄ±ndaki metni yakala
    # FarklÄ± tire karakterlerini (â€”, -, â€“, :, ;) destekle
    # NoktalÄ± virgÃ¼l (;) bazÄ± makalelerde kullanÄ±lÄ±yor
    pattern = r"Anahtar\s*Kelimeler\s*[â€”:\-â€“;]+\s*(.*?)\s*(?=Abstract)"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    return clean_text(match.group(1)) if match else ""


def extract_keywords_en(text: str) -> str:
    """
    Ä°ngilizce anahtar kelimeleri "Keywordsâ€”" ile "GiriÅŸ" veya "Problem TanÄ±mÄ±" bÃ¶lÃ¼mÃ¼ arasÄ±ndan Ã§Ä±karÄ±r.
    
    Anahtar kelimeler genellikle Ä°ngilizce Ã¶zetin hemen ardÄ±ndan gelir ve virgÃ¼lle ayrÄ±lmÄ±ÅŸ
    kelime/kelime gruplarÄ± ÅŸeklindedir. GiriÅŸ bÃ¶lÃ¼mÃ¼ farklÄ± formatlarda olabilir:
    - "I." veya "I " (Roma rakamÄ±) - yeni satÄ±rda veya aynÄ± satÄ±rda olabilir
    - "GÄ°RÄ°Å" (TÃ¼rkÃ§e)
    - "INTRODUCTION" (Ä°ngilizce)
    - "PROBLEM" veya "PROBLEMÄ°N TANIMI" gibi varyasyonlar
    - Bazen hiÃ§bir baÅŸlÄ±k olmayabilir, bu durumda ilk satÄ±rÄ± al
    
    Args:
        text: Ä°ÅŸlenecek metin
        
    Returns:
        Ä°ngilizce anahtar kelimeler (virgÃ¼lle ayrÄ±lmÄ±ÅŸ)
    """
    # "Keywords" ile giriÅŸ bÃ¶lÃ¼mÃ¼ arasÄ±ndaki metni yakala
    # FarklÄ± tire karakterlerini (â€”, -, â€“, :, ;) destekle
    # NoktalÄ± virgÃ¼l (;) bazÄ± makalelerde kullanÄ±lÄ±yor
    # GiriÅŸ bÃ¶lÃ¼mÃ¼ iÃ§in birden fazla pattern kontrol et:
    # - \n\s*I\. : yeni satÄ±rda "I." (eski pattern)
    # - I\.\s : "I." sonrasÄ± boÅŸluk (I. GÄ°RÄ°Å, I. PROBLEM gibi aynÄ± satÄ±rda)
    # - PROBLEM : "PROBLEM TANIMI", "PROBLEMÄ°N TANIMI" vb. iÃ§in genel pattern
    pattern = r"Keywords\s*[â€”:\-â€“;]+\s*(.*?)(?=\n\s*I\.|I\.\s|GÄ°RÄ°Å|INTRODUCTION|PROBLEM|\n\s*\n\s*[A-Z][a-z]+)"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    
    if match:
        result = clean_text(match.group(1))
        # EÄŸer sonuÃ§ Ã§ok uzunsa (>200 karakter), muhtemelen yanlÄ±ÅŸ yakalanmÄ±ÅŸtÄ±r, sadece ilk cÃ¼mleyi al
        if len(result) > 200:
            # Nokta veya Ã§ift newline'da kes
            parts = result.split('.')
            if parts:
                result = clean_text(parts[0] + '.')
        return result
    
    # EÄŸer yukarÄ±daki pattern baÅŸarÄ±sÄ±z olursa, daha basit bir yÃ¶ntem dene
    # Keywords'den sonra ilk satÄ±rÄ± al (Ã§ift newline'a kadar)
    simple_pattern = r"Keywords\s*[â€”:\-â€“;]+\s*([^\n]+)"
    simple_match = re.search(simple_pattern, text, re.IGNORECASE)
    if simple_match:
        return clean_text(simple_match.group(1))
    
    return ""


# ====================================================================
# TITLE EXTRACTION - BaÅŸlÄ±k Ã§Ä±karma fonksiyonlarÄ±
# ====================================================================

def _filter_noise_spans(spans: list, page_height: float) -> list:
    """
    BaÅŸlÄ±k aday span'lerinden gÃ¼rÃ¼ltÃ¼yÃ¼ filtreler (header, footer, vb).
    
    Args:
        spans: Span listesi
        page_height: Sayfa yÃ¼ksekliÄŸi
        
    Returns:
        FiltrelenmiÅŸ span listesi
    """
    filtered = []
    for s in spans:
        text = s["text"]
        
        # Ã‡ok kÄ±sa veya boÅŸ metinleri atla
        if len(text) < 2:
            continue
            
        # Kitap baÅŸlÄ±ÄŸÄ±/header bilgilerini atla
        if "LIFT UP" in text or "Bildiri KitabÄ±" in text:
            continue
            
        # Email adresleri atla
        if "@" in text:
            continue
            
        filtered.append(s)
    
    return filtered


def _group_spans_into_lines(spans: list, y_tolerance: float = 3.0) -> list:
    """
    Span'leri Y pozisyonuna gÃ¶re satÄ±rlara gruplar.
    
    Args:
        spans: Span listesi (Y'ye gÃ¶re sÄ±ralanmÄ±ÅŸ olmalÄ±)
        y_tolerance: AynÄ± satÄ±rdaki span'ler iÃ§in Y toleransÄ±
        
    Returns:
        SatÄ±r listesi [{"y": avg_y, "text": line_text}, ...]
    """
    lines = []
    current_line = []
    current_y = None
    
    def flush_line():
        """Mevcut satÄ±rÄ± lines listesine ekle"""
        if not current_line:
            return
        current_line.sort(key=lambda d: d["x"])
        line_text = clean_text(" ".join(d["text"] for d in current_line))
        if line_text:
            avg_y = sum(d["y"] for d in current_line) / len(current_line)
            lines.append({"y": avg_y, "text": line_text})
    
    for span in spans:
        if current_y is None:
            current_y = span["y"]
            current_line = [span]
            continue
        
        # AynÄ± satÄ±rda mÄ±?
        if abs(span["y"] - current_y) <= y_tolerance:
            current_line.append(span)
        else:
            # Yeni satÄ±r baÅŸladÄ±
            flush_line()
            current_y = span["y"]
            current_line = [span]
    
    # Son satÄ±rÄ± ekle
    flush_line()
    
    return lines


def _filter_non_title_lines(lines: list) -> list:
    """
    BaÅŸlÄ±k olmayan satÄ±rlarÄ± filtreler (yazar bilgileri, kurumlar, vb).
    
    Args:
        lines: SatÄ±r listesi
        
    Returns:
        FiltrelenmiÅŸ satÄ±r listesi
    """
    filtered = []
    
    for line in lines:
        text = line["text"]
        
        # Ã–zet bÃ¶lÃ¼mÃ¼ne geldiysek dur
        if "Ã–zetÃ§e" in text or "Abstract" in text:
            break
        
        # Yazar/kurum bilgileri
        if text.startswith("Ã–ÄŸrenci") or text.startswith("Akademik DanÄ±ÅŸman") or text.startswith("Sanayi DanÄ±ÅŸmanÄ±"):
            break
        
        # Email, ÅŸehir, ÅŸirket bilgileri
        if "@" in text:
            break
        if text in ["Ankara, TÃ¼rkiye", "Ä°stanbul, TÃ¼rkiye", "TÃ¼rkiye", "Turkey"]:
            break
        if "A.Å." in text:
            break
        
        # Ã‡ok kÄ±sa satÄ±rlarÄ± atla
        if len(text) < 3:
            continue
        
        filtered.append(line)
        
        # BaÅŸlÄ±k Ã§ok uzun olmasÄ±n (maksimum 12 satÄ±r)
        if len(filtered) >= 12:
            break
    
    return filtered


def _split_tr_en_by_gap(texts: list, ys: list, gap_threshold: float = 8.0) -> tuple[str, str]:
    """
    SatÄ±rlar arasÄ±ndaki gap'e (boÅŸluk) bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r.
    
    Args:
        texts: SatÄ±r metinleri listesi
        ys: SatÄ±rlarÄ±n Y pozisyonlarÄ±
        gap_threshold: Minimum gap boyutu
        
    Returns:
        (title_tr, title_en) tuple
    """
    if len(ys) < 2:
        return "", ""
    
    # ArdÄ±ÅŸÄ±k satÄ±rlar arasÄ±ndaki gap'leri hesapla
    gaps = [ys[i + 1] - ys[i] for i in range(len(ys) - 1)]
    max_gap = max(gaps)
    max_gap_idx = gaps.index(max_gap)
    
    # Yeterince bÃ¼yÃ¼k gap yoksa None dÃ¶ndÃ¼r
    if max_gap < gap_threshold:
        return "", ""
    
    # Gap'e gÃ¶re bÃ¶l
    split_idx = max_gap_idx + 1
    top_texts = texts[:split_idx]
    bottom_texts = texts[split_idx:]
    
    # Hangi grup daha Ã§ok Ä°ngilizce ipucu iÃ§eriyor?
    top_en_score = sum(1 for t in top_texts if looks_english_line(t))
    bottom_en_score = sum(1 for t in bottom_texts if looks_english_line(t))
    
    if bottom_en_score >= top_en_score:
        return clean_text(" ".join(top_texts)), clean_text(" ".join(bottom_texts))
    else:
        return clean_text(" ".join(bottom_texts)), clean_text(" ".join(top_texts))


def _split_tr_en_by_english_hint(texts: list) -> tuple[str, str]:
    """
    Ä°ngilizce ipuÃ§larÄ±na bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r.
    
    Args:
        texts: SatÄ±r metinleri listesi
        
    Returns:
        (title_tr, title_en) tuple
    """
    first_en_idx = None
    for i, text in enumerate(texts):
        if looks_english_line(text):
            first_en_idx = i
            break
    
    if first_en_idx is not None and first_en_idx > 0:
        title_tr = clean_text(" ".join(texts[:first_en_idx]))
        title_en = clean_text(" ".join(texts[first_en_idx:]))
        return title_tr, title_en
    
    return "", ""


def _split_tr_en_by_char(texts: list) -> tuple[str, str]:
    """
    TÃ¼rkÃ§e karakter varlÄ±ÄŸÄ±na bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r (fallback method).
    
    Args:
        texts: SatÄ±r metinleri listesi
        
    Returns:
        (title_tr, title_en) tuple
    """
    tr_lines = []
    en_lines = []
    found_en = False
    
    for text in texts:
        # TÃ¼rkÃ§e karakter var ve henÃ¼z EN baÅŸlamadÄ±ysa -> TR
        if contains_tr_char(text) and not found_en:
            tr_lines.append(text)
            continue
        
        # TR bittikten sonra TÃ¼rkÃ§e karakter yok -> EN baÅŸladÄ±
        if not contains_tr_char(text) and tr_lines:
            found_en = True
            en_lines.append(text)
            continue
        
        # Belirsiz durumlar
        if not tr_lines and not found_en:
            tr_lines.append(text)
        else:
            en_lines.append(text)
    
    return clean_text(" ".join(tr_lines)), clean_text(" ".join(en_lines))


def extract_title_tr_en(page) -> tuple[str, str]:
    """
    Sayfadan makale baÅŸlÄ±ÄŸÄ±nÄ± TÃ¼rkÃ§e ve Ä°ngilizce olarak ayrÄ± Ã§Ä±karÄ±r.
    
    ÃœÃ§ aÅŸamalÄ± ayÄ±rma stratejisi:
    1. Gap-based: SatÄ±rlar arasÄ± â‰¥8pt boÅŸluk varsa bunu TR/EN sÄ±nÄ±rÄ± kabul et
    2. English hint: Ä°ngilizce kelime ipuÃ§larÄ±na gÃ¶re ayÄ±r
    3. Character-based: TÃ¼rkÃ§e karakter varlÄ±ÄŸÄ±na gÃ¶re ayÄ±r (fallback)
    
    Font boyutu toleransÄ±: max_size - 4pt (baÅŸlÄ±ÄŸÄ±n ilk satÄ±rlarÄ± bazen daha kÃ¼Ã§Ã¼k olabiliyor)
    
    Args:
        page: PyMuPDF page objesi
        
    Returns:
        (title_tr, title_en) tuple
    """
    info = page.get_text("dict")
    page_h = float(page.rect.height)

    # 1. TÃ¼m span'leri topla
    spans = []
    for block in info.get("blocks", []):
        for line in block.get("lines", []):
            for sp in line.get("spans", []):
                txt = (sp.get("text") or "").strip()
                if not txt or len(txt) < 2:
                    continue
                x0, y0, x1, y1 = sp.get("bbox", [0, 0, 0, 0])
                spans.append({
                    "text": txt,
                    "x": float(x0),
                    "y": float(y0),
                    "size": float(sp.get("size", 0.0)),
                })

    if not spans:
        return "", ""

    # 2. Ã–zetÃ§e/Abstract'Ä±n Y pozisyonunu bul (baÅŸlÄ±k alanÄ±nÄ±n alt sÄ±nÄ±rÄ±)
    y_abstract = None
    for s in spans:
        if "Ã–zetÃ§e" in s["text"] or "Abstract" in s["text"]:
            if y_abstract is None or s["y"] < y_abstract:
                y_abstract = s["y"]
    if y_abstract is None:
        y_abstract = page_h * 0.60

    # 3. BaÅŸlÄ±k bÃ¶lgesini belirle (sayfanÄ±n Ã¼stÃ¼nden Ã¶zete kadar)
    y_max = y_abstract - 2
    region = [s for s in spans if s["y"] <= y_max]
    if not region:
        return "", ""

    # 4. En bÃ¼yÃ¼k fontu bul ve tolerans bandÄ± seÃ§
    max_size = max(s["size"] for s in region)
    if max_size <= 0:
        return "", ""
    
    # Font toleransÄ±: max_size - 4pt (baÅŸlÄ±ÄŸÄ±n ilk satÄ±rlarÄ± bazen daha kÃ¼Ã§Ã¼k)
    band = [s for s in region if s["size"] >= max_size - 4.0]
    if not band:
        return "", ""

    # 5. Span'leri Y pozisyonuna gÃ¶re sÄ±rala
    band.sort(key=lambda d: (d["y"], d["x"]))
    
    # 6. GÃ¼rÃ¼ltÃ¼yÃ¼ filtrele
    band = _filter_noise_spans(band, page_h)
    if not band:
        return "", ""

    # 7. Span'leri satÄ±rlara grupla
    lines = _group_spans_into_lines(band, y_tolerance=3.0)
    if not lines:
        return "", ""

    # 8. BaÅŸlÄ±k olmayan satÄ±rlarÄ± filtrele
    lines = _filter_non_title_lines(lines)
    if not lines:
        return "", ""

    # 9. Y pozisyonuna gÃ¶re sÄ±rala
    lines.sort(key=lambda d: d["y"])
    texts = [line["text"] for line in lines]
    ys = [line["y"] for line in lines]

    # 10. ÃœÃ§ aÅŸamalÄ± ayÄ±rma stratejisi
    
    # Strateji 1: Gap ile ayÄ±r
    title_tr, title_en = _split_tr_en_by_gap(texts, ys, gap_threshold=8.0)
    if title_tr and title_en:
        return title_tr, title_en

    # Strateji 2: Ä°ngilizce ipuÃ§larÄ±na gÃ¶re ayÄ±r
    title_tr, title_en = _split_tr_en_by_english_hint(texts)
    if title_tr and title_en:
        return title_tr, title_en

    # Strateji 3: TÃ¼rkÃ§e karakter varlÄ±ÄŸÄ±na gÃ¶re ayÄ±r (fallback)
    return _split_tr_en_by_char(texts)


# ====================================================================
# PDF PROCESSING - Ana iÅŸleme fonksiyonlarÄ±
# ====================================================================

def extract_abstracts_with_fallback(doc, page_idx: int) -> tuple[str, str]:
    """
    Ã–zet Ã§Ä±karÄ±mÄ±nÄ± birden fazla stratejiyle dener (bazÄ± Ã¶zetler birkaÃ§ sayfaya yayÄ±labilir).
    
    Args:
        doc: PDF dÃ¶kÃ¼manÄ±
        page_idx: Makale baÅŸlangÄ±Ã§ sayfasÄ± indeksi
        
    Returns:
        (abstract_tr, abstract_en) tuple
    """
    # Ä°lk deneme: Normal marker'larla
    merged_tr = collect_until_markers(doc, page_idx, ["Anahtar Kelimeler"], hard_limit=8)
    merged_en = collect_until_markers(doc, page_idx, ["Keywords"], hard_limit=8)
    abs_tr = extract_abstract_tr(merged_tr)
    abs_en = extract_abstract_en(merged_en)

    # TÃ¼rkÃ§e Ã¶zet bulunamadÄ±ysa alternatif deneme
    if not abs_tr:
        merged_tr2 = collect_until_markers(doc, page_idx, ["Abstract", "Keywords"], hard_limit=8)
        match = re.search(
            r"Ã–zetÃ§e\s*[â€”\-â€“]+\s*(.*?)\s*(?=Abstract|Keywords)",
            merged_tr2,
            re.DOTALL | re.IGNORECASE
        )
        abs_tr = clean_text(match.group(1)) if match else ""

    # Ä°ngilizce Ã¶zet bulunamadÄ±ysa alternatif deneme
    if not abs_en:
        merged_en2 = collect_until_markers(doc, page_idx, ["I.", "I ", "GÄ°RÄ°Å"], hard_limit=8)
        match = re.search(
            r"Abstract\s*[â€”\-â€“]+\s*(.*?)\s*(?=Keywords|I\.\s|I\s|GÄ°RÄ°Å)",
            merged_en2,
            re.DOTALL | re.IGNORECASE
        )
        abs_en = clean_text(match.group(1)) if match else ""

    return abs_tr, abs_en


def process_pdf(pdf_path: str, year: str, output_csv: str | None = None):
    """
    Tek bir PDF dosyasÄ±ndan tÃ¼m makaleleri Ã§Ä±karÄ±r ve CSV'ye yazar.
    
    Args:
        pdf_path: PDF dosya yolu
        year: YÄ±l bilgisi (CSV'ye yazÄ±lacak)
        output_csv: Ã‡Ä±ktÄ± CSV dosya yolu (None ise otomatik oluÅŸturulur)
    """
    print(f"ğŸ“„ PDF aÃ§Ä±lÄ±yor: {pdf_path}")
    doc = fitz.open(pdf_path)
    print(f"ğŸ“Š Toplam sayfa sayÄ±sÄ±: {len(doc)}")

    rows = []

    # Her sayfayÄ± tara
    for page_idx in range(len(doc)):
        page = doc[page_idx]
        text = page.get_text()

        # Bu sayfa yeni makale baÅŸlangÄ±cÄ± mÄ±?
        if not is_article_start_page(text):
            continue

        # BaÅŸlÄ±klarÄ± Ã§Ä±kar (TR ve EN ayrÄ±)
        title_tr, title_en = extract_title_tr_en(page)

        # Ã–zetleri Ã§Ä±kar (fallback stratejileriyle)
        abs_tr, abs_en = extract_abstracts_with_fallback(doc, page_idx)

        # Anahtar kelimeleri Ã§Ä±kar (TR ve EN ayrÄ±)
        # Anahtar kelimeler iÃ§in sayfa metnini topla (birkaÃ§ sayfaya yayÄ±labilir)
        keywords_text = collect_until_markers(doc, page_idx, ["I.", "GÄ°RÄ°Å", "INTRODUCTION"], hard_limit=3)
        keywords_tr = extract_keywords_tr(keywords_text)
        keywords_en = extract_keywords_en(keywords_text)

        # Makale bilgilerini kaydet
        rows.append({
            "PageNumber": page_idx + 1,
            "Year": year,
            "Title_TR": title_tr,
            "Title_EN": title_en,
            "Abstract_TR": abs_tr,
            "Abstract_EN": abs_en,
            "Keywords_TR": keywords_tr,
            "Keywords_EN": keywords_en,
        })

        # Ä°lerleme gÃ¶ster
        print(f"âœ… Sayfa {page_idx+1}: TR='{title_tr[:60]}...' | EN='{title_en[:60]}...'")

    doc.close()

    # CSV dosya adÄ±nÄ± belirle
    if output_csv is None:
        base = os.path.splitext(pdf_path)[0]
        output_csv = base + ".csv"

    # CSV'ye yaz
    with open(output_csv, "w", newline="", encoding="utf-8-sig") as f:
        fieldnames = ["PageNumber", "Year", "Title_TR", "Title_EN", "Abstract_TR", "Abstract_EN", "Keywords_TR", "Keywords_EN"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)

    print(f"\nâœ¨ {len(rows)} makale bulundu. CSV yazÄ±ldÄ±: {output_csv}")


def process_path(input_path: str, year: str, out_dir: str | None = None):
    """
    PDF dosyasÄ±, klasÃ¶r veya glob pattern'i iÅŸler.
    
    Args:
        input_path: PDF dosyasÄ±, klasÃ¶r yolu veya glob pattern (Ã¶rn: "2021-2022/*.pdf")
        year: YÄ±l bilgisi
        out_dir: Ã‡Ä±ktÄ± dizini (None ise PDF ile aynÄ± yerde oluÅŸturulur)
        
    Raises:
        FileNotFoundError: PDF bulunamazsa
    """
    pdfs = []
    
    # KlasÃ¶r mÃ¼, dosya mÄ±, glob pattern mi?
    if os.path.isdir(input_path):
        pdfs = sorted(glob.glob(os.path.join(input_path, "*.pdf")))
    else:
        matches = glob.glob(input_path)
        if matches:
            pdfs = sorted([p for p in matches if p.lower().endswith(".pdf")])
        elif input_path.lower().endswith(".pdf"):
            pdfs = [input_path]

    if not pdfs:
        raise FileNotFoundError(f"âŒ PDF bulunamadÄ±: {input_path}")

    print(f"\nğŸ” {len(pdfs)} PDF dosyasÄ± bulundu\n")

    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    if out_dir is not None:
        os.makedirs(out_dir, exist_ok=True)

    # Her PDF'i iÅŸle
    for idx, pdf in enumerate(pdfs, 1):
        print(f"\n{'='*80}")
        print(f"[{idx}/{len(pdfs)}] Ä°ÅŸleniyor...")
        print(f"{'='*80}")
        
        if out_dir:
            out_csv = os.path.join(out_dir, os.path.splitext(os.path.basename(pdf))[0] + ".csv")
        else:
            out_csv = None
        
        process_pdf(pdf, year, out_csv)


# ====================================================================
# MAIN EXECUTION
# ====================================================================

if __name__ == "__main__":
    print("="*80)
    print("LIFT UP Dataset Extraction Tool")
    print("="*80)
    print(f"PDF Path: {PDF_PATH}")
    print(f"Year: {YEAR}")
    print(f"Output Dir: {OUTPUT_DIR}")
    print("="*80 + "\n")
    
    try:
        process_path(PDF_PATH, YEAR, OUTPUT_DIR)
        print("\nğŸ‰ Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
