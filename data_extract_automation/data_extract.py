import fitz
import re
import csv
import os
import glob
from typing import Optional, Tuple, List, Dict
from dataclasses import dataclass


# ====================================================================
# DATA MODELS
# ====================================================================

@dataclass
class Article:
    """Makale verilerini tutan veri modeli"""
    page_number: int
    year: str
    title_tr: str
    title_en: str
    abstract_tr: str
    abstract_en: str
    keywords_tr: str
    keywords_en: str
    
    def to_dict(self) -> Dict[str, any]:
        """Makale verisini dictionary'ye Ã§evirir (CSV iÃ§in)"""
        return {
            "PageNumber": self.page_number,
            "Year": self.year,
            "Title_TR": self.title_tr,
            "Title_EN": self.title_en,
            "Abstract_TR": self.abstract_tr,
            "Abstract_EN": self.abstract_en,
            "Keywords_TR": self.keywords_tr,
            "Keywords_EN": self.keywords_en,
        }


# ====================================================================
# TEXT UTILITIES
# ====================================================================

class TextUtils:
    """Metin iÅŸleme yardÄ±mcÄ± sÄ±nÄ±fÄ±"""
    
    TR_CHARS = set("ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅÄ°Ã–Ã‡")
    
    @staticmethod
    def clean_text(text: str) -> str:
        """
        Metindeki fazla boÅŸluklarÄ± temizler ve strip yapar.
        
        Args:
            text: Temizlenecek metin
            
        Returns:
            TemizlenmiÅŸ metin
        """
        if not text:
            return ""
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    
    @classmethod
    def contains_tr_char(cls, text: str) -> bool:
        """
        Metinde TÃ¼rkÃ§e karakter olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        
        Args:
            text: Kontrol edilecek metin
            
        Returns:
            True/False
        """
        return any(c in cls.TR_CHARS for c in text)
    
    @classmethod
    def looks_english_line(cls, text: str) -> bool:
        """
        SatÄ±rÄ±n Ä°ngilizce baÅŸlÄ±k satÄ±rÄ± olup olmadÄ±ÄŸÄ±nÄ± heuristik yÃ¶ntemle kontrol eder.
        
        Args:
            text: Kontrol edilecek satÄ±r
            
        Returns:
            True ise muhtemelen Ä°ngilizce satÄ±r
        """
        if not text:
            return False
        if cls.contains_tr_char(text):
            return False
        
        low = text.lower()
        english_hints = [
            "production", "testing", "used in", "using", "technology",
            "system", "systems", "analysis", "design", "optimization",
            "manufacturing", "additive"
        ]
        return any(hint in low for hint in english_hints)


# ====================================================================
# PAGE ANALYZER
# ====================================================================

class PageAnalyzer:
    """Sayfa analizi ve iÃ§erik tespiti sÄ±nÄ±fÄ±"""
    
    @staticmethod
    def is_article_start_page(text: str) -> bool:
        """
        SayfanÄ±n yeni bir makale baÅŸlangÄ±cÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        
        Args:
            text: Sayfa metni
            
        Returns:
            True ise yeni makale baÅŸlangÄ±cÄ±
        """
        return ("Ã–zetÃ§e" in text) and ("Abstract" in text)
    
    @staticmethod
    def collect_until_markers(doc, start_idx: int, stop_markers: List[str], 
                            hard_limit: int = 8) -> str:
        """
        Belirli bir sayfadan baÅŸlayarak, durma iÅŸaretÃ§ilerine kadar olan metni toplar.
        
        Args:
            doc: PDF dÃ¶kÃ¼manÄ±
            start_idx: BaÅŸlangÄ±Ã§ sayfa indeksi
            stop_markers: Durma iÅŸaretÃ§ileri listesi
            hard_limit: Maksimum kaÃ§ sayfa toplanacak
            
        Returns:
            BirleÅŸtirilmiÅŸ metin
        """
        parts = []
        analyzer = PageAnalyzer()
        
        for i in range(start_idx, min(len(doc), start_idx + hard_limit)):
            page_text = doc[i].get_text()
            
            # Yeni makale baÅŸladÄ±ysa dur
            if i > start_idx and analyzer.is_article_start_page(page_text):
                break
                
            parts.append(page_text)
            
            # Durma iÅŸaretÃ§ilerini kontrol et
            low = page_text.lower()
            if any(marker.lower() in low for marker in stop_markers):
                break
                
        return "\n".join(parts)


# ====================================================================
# ABSTRACT EXTRACTOR
# ====================================================================

class AbstractExtractor:
    """Ã–zet ve anahtar kelime Ã§Ä±karma sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.text_utils = TextUtils()
    
    def extract_abstract_tr(self, text: str) -> str:
        """TÃ¼rkÃ§e Ã¶zeti Ã§Ä±karÄ±r"""
        pattern = r"Ã–zetÃ§e\s*[â€”\-â€“]+\s*(.*?)\s*(?=Anahtar\s*Kelimeler)"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        return self.text_utils.clean_text(match.group(1)) if match else ""
    
    def extract_abstract_en(self, text: str) -> str:
        """Ä°ngilizce Ã¶zeti Ã§Ä±karÄ±r"""
        pattern = r"Abstract\s*[â€”\-â€“]+\s*(.*?)\s*(?=Keywords)"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        return self.text_utils.clean_text(match.group(1)) if match else ""
    
    def extract_keywords_tr(self, text: str) -> str:
        """TÃ¼rkÃ§e anahtar kelimeleri Ã§Ä±karÄ±r"""
        pattern = r"Anahtar\s*Kelimeler\s*[â€”:\-â€“;]+\s*(.*?)\s*(?=Abstract)"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        return self.text_utils.clean_text(match.group(1)) if match else ""
    
    def extract_keywords_en(self, text: str) -> str:
        """Ä°ngilizce anahtar kelimeleri Ã§Ä±karÄ±r"""
        pattern = r"Keywords\s*[â€”:\-â€“;]+\s*(.*?)(?=\n\s*I\.|I\.\s|GÄ°RÄ°Å|INTRODUCTION|PROBLEM|\n\s*\n\s*[A-Z][a-z]+)"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        
        if match:
            result = self.text_utils.clean_text(match.group(1))
            # Ã‡ok uzunsa ilk cÃ¼mleyi al
            if len(result) > 200:
                parts = result.split('.')
                if parts:
                    result = self.text_utils.clean_text(parts[0] + '.')
            return result
        
        # Basit pattern dene
        simple_pattern = r"Keywords\s*[â€”:\-â€“;]+\s*([^\n]+)"
        simple_match = re.search(simple_pattern, text, re.IGNORECASE)
        if simple_match:
            return self.text_utils.clean_text(simple_match.group(1))
        
        return ""
    
    def extract_with_fallback(self, doc, page_idx: int) -> Tuple[str, str, str, str]:
        """
        Ã–zet ve anahtar kelimeleri fallback stratejileriyle Ã§Ä±karÄ±r.
        
        Returns:
            (abstract_tr, abstract_en, keywords_tr, keywords_en) tuple
        """
        page_analyzer = PageAnalyzer()
        
        # Ã–zetleri Ã§Ä±kar
        merged_tr = page_analyzer.collect_until_markers(
            doc, page_idx, ["Anahtar Kelimeler"], hard_limit=8
        )
        merged_en = page_analyzer.collect_until_markers(
            doc, page_idx, ["Keywords"], hard_limit=8
        )
        abs_tr = self.extract_abstract_tr(merged_tr)
        abs_en = self.extract_abstract_en(merged_en)
        
        # TÃ¼rkÃ§e Ã¶zet fallback
        if not abs_tr:
            merged_tr2 = page_analyzer.collect_until_markers(
                doc, page_idx, ["Abstract", "Keywords"], hard_limit=8
            )
            match = re.search(
                r"Ã–zetÃ§e\s*[â€”\-â€“]+\s*(.*?)\s*(?=Abstract|Keywords)",
                merged_tr2,
                re.DOTALL | re.IGNORECASE
            )
            abs_tr = self.text_utils.clean_text(match.group(1)) if match else ""
        
        # Ä°ngilizce Ã¶zet fallback
        if not abs_en:
            merged_en2 = page_analyzer.collect_until_markers(
                doc, page_idx, ["I.", "I ", "GÄ°RÄ°Å"], hard_limit=8
            )
            match = re.search(
                r"Abstract\s*[â€”\-â€“]+\s*(.*?)\s*(?=Keywords|I\.\s|I\s|GÄ°RÄ°Å)",
                merged_en2,
                re.DOTALL | re.IGNORECASE
            )
            abs_en = self.text_utils.clean_text(match.group(1)) if match else ""
        
        # Anahtar kelimeleri Ã§Ä±kar
        keywords_text = page_analyzer.collect_until_markers(
            doc, page_idx, ["I.", "GÄ°RÄ°Å", "INTRODUCTION"], hard_limit=3
        )
        keywords_tr = self.extract_keywords_tr(keywords_text)
        keywords_en = self.extract_keywords_en(keywords_text)
        
        return abs_tr, abs_en, keywords_tr, keywords_en


# ====================================================================
# TITLE EXTRACTOR
# ====================================================================

class TitleExtractor:
    """BaÅŸlÄ±k Ã§Ä±karma sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.text_utils = TextUtils()
    
    def _filter_noise_spans(self, spans: List[Dict], page_height: float) -> List[Dict]:
        """BaÅŸlÄ±k aday span'lerinden gÃ¼rÃ¼ltÃ¼yÃ¼ filtreler"""
        filtered = []
        for s in spans:
            text = s["text"]
            
            # Ã‡ok kÄ±sa veya boÅŸ metinleri atla
            if len(text) < 2:
                continue
                
            # Header bilgilerini atla
            if "LIFT UP" in text or "Bildiri KitabÄ±" in text:
                continue
                
            # Email adresleri atla
            if "@" in text:
                continue
                
            filtered.append(s)
        
        return filtered
    
    def _group_spans_into_lines(self, spans: List[Dict], y_tolerance: float = 3.0) -> List[Dict]:
        """Span'leri Y pozisyonuna gÃ¶re satÄ±rlara gruplar"""
        lines = []
        current_line = []
        current_y = None
        
        def flush_line():
            """Mevcut satÄ±rÄ± lines listesine ekle"""
            if not current_line:
                return
            current_line.sort(key=lambda d: d["x"])
            line_text = self.text_utils.clean_text(" ".join(d["text"] for d in current_line))
            if line_text:
                avg_y = sum(d["y"] for d in current_line) / len(current_line)
                lines.append({"y": avg_y, "text": line_text})
        
        for span in spans:
            if current_y is None:
                current_y = span["y"]
                current_line = [span]
                continue
            
            # AynÄ± satÄ±rda mÄ±?
            if abs(span["y"] - current_y) <= y_tolerance:
                current_line.append(span)
            else:
                # Yeni satÄ±r baÅŸladÄ±
                flush_line()
                current_y = span["y"]
                current_line = [span]
        
        # Son satÄ±rÄ± ekle
        flush_line()
        
        return lines
    
    def _filter_non_title_lines(self, lines: List[Dict]) -> List[Dict]:
        """BaÅŸlÄ±k olmayan satÄ±rlarÄ± filtreler"""
        filtered = []
        
        for line in lines:
            text = line["text"]
            
            # Ã–zet bÃ¶lÃ¼mÃ¼ne geldiysek dur
            if "Ã–zetÃ§e" in text or "Abstract" in text:
                break
            
            # Yazar/kurum bilgileri
            if text.startswith("Ã–ÄŸrenci") or text.startswith("Akademik DanÄ±ÅŸman") or text.startswith("Sanayi DanÄ±ÅŸmanÄ±"):
                break
            
            # Email, ÅŸehir, ÅŸirket bilgileri
            if "@" in text:
                break
            if text in ["Ankara, TÃ¼rkiye", "Ä°stanbul, TÃ¼rkiye", "TÃ¼rkiye", "Turkey"]:
                break
            if "A.Å." in text:
                break
            
            # Ã‡ok kÄ±sa satÄ±rlarÄ± atla
            if len(text) < 3:
                continue
            
            filtered.append(line)
            
            # BaÅŸlÄ±k Ã§ok uzun olmasÄ±n (maksimum 12 satÄ±r)
            if len(filtered) >= 12:
                break
        
        return filtered
    
    def _split_tr_en_by_gap(self, texts: List[str], ys: List[float], 
                           gap_threshold: float = 8.0) -> Tuple[str, str]:
        """SatÄ±rlar arasÄ±ndaki gap'e bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r"""
        if len(ys) < 2:
            return "", ""
        
        # ArdÄ±ÅŸÄ±k satÄ±rlar arasÄ±ndaki gap'leri hesapla
        gaps = [ys[i + 1] - ys[i] for i in range(len(ys) - 1)]
        max_gap = max(gaps)
        max_gap_idx = gaps.index(max_gap)
        
        # Yeterince bÃ¼yÃ¼k gap yoksa boÅŸ dÃ¶ndÃ¼r
        if max_gap < gap_threshold:
            return "", ""
        
        # Gap'e gÃ¶re bÃ¶l
        split_idx = max_gap_idx + 1
        top_texts = texts[:split_idx]
        bottom_texts = texts[split_idx:]
        
        # Hangi grup daha Ã§ok Ä°ngilizce ipucu iÃ§eriyor?
        top_en_score = sum(1 for t in top_texts if self.text_utils.looks_english_line(t))
        bottom_en_score = sum(1 for t in bottom_texts if self.text_utils.looks_english_line(t))
        
        if bottom_en_score >= top_en_score:
            return self.text_utils.clean_text(" ".join(top_texts)), self.text_utils.clean_text(" ".join(bottom_texts))
        else:
            return self.text_utils.clean_text(" ".join(bottom_texts)), self.text_utils.clean_text(" ".join(top_texts))
    
    def _split_tr_en_by_english_hint(self, texts: List[str]) -> Tuple[str, str]:
        """Ä°ngilizce ipuÃ§larÄ±na bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r"""
        first_en_idx = None
        for i, text in enumerate(texts):
            if self.text_utils.looks_english_line(text):
                first_en_idx = i
                break
        
        if first_en_idx is not None and first_en_idx > 0:
            title_tr = self.text_utils.clean_text(" ".join(texts[:first_en_idx]))
            title_en = self.text_utils.clean_text(" ".join(texts[first_en_idx:]))
            return title_tr, title_en
        
        return "", ""
    
    def _split_tr_en_by_char(self, texts: List[str]) -> Tuple[str, str]:
        """TÃ¼rkÃ§e karakter varlÄ±ÄŸÄ±na bakarak TR ve EN baÅŸlÄ±klarÄ± ayÄ±rÄ±r"""
        tr_lines = []
        en_lines = []
        found_en = False
        
        for text in texts:
            # TÃ¼rkÃ§e karakter var ve henÃ¼z EN baÅŸlamadÄ±ysa -> TR
            if self.text_utils.contains_tr_char(text) and not found_en:
                tr_lines.append(text)
                continue
            
            # TR bittikten sonra TÃ¼rkÃ§e karakter yok -> EN baÅŸladÄ±
            if not self.text_utils.contains_tr_char(text) and tr_lines:
                found_en = True
                en_lines.append(text)
                continue
            
            # Belirsiz durumlar
            if not tr_lines and not found_en:
                tr_lines.append(text)
            else:
                en_lines.append(text)
        
        return self.text_utils.clean_text(" ".join(tr_lines)), self.text_utils.clean_text(" ".join(en_lines))
    
    def extract(self, page) -> Tuple[str, str]:
        """
        Sayfadan makale baÅŸlÄ±ÄŸÄ±nÄ± TÃ¼rkÃ§e ve Ä°ngilizce olarak ayrÄ± Ã§Ä±karÄ±r.
        
        Args:
            page: PyMuPDF page objesi
            
        Returns:
            (title_tr, title_en) tuple
        """
        info = page.get_text("dict")
        page_h = float(page.rect.height)
        
        # 1. TÃ¼m span'leri topla
        spans = []
        for block in info.get("blocks", []):
            for line in block.get("lines", []):
                for sp in line.get("spans", []):
                    txt = (sp.get("text") or "").strip()
                    if not txt or len(txt) < 2:
                        continue
                    x0, y0, x1, y1 = sp.get("bbox", [0, 0, 0, 0])
                    spans.append({
                        "text": txt,
                        "x": float(x0),
                        "y": float(y0),
                        "size": float(sp.get("size", 0.0)),
                    })
        
        if not spans:
            return "", ""
        
        # 2. Ã–zetÃ§e/Abstract'Ä±n Y pozisyonunu bul
        y_abstract = None
        for s in spans:
            if "Ã–zetÃ§e" in s["text"] or "Abstract" in s["text"]:
                if y_abstract is None or s["y"] < y_abstract:
                    y_abstract = s["y"]
        if y_abstract is None:
            y_abstract = page_h * 0.60
        
        # 3. BaÅŸlÄ±k bÃ¶lgesini belirle
        y_max = y_abstract - 2
        region = [s for s in spans if s["y"] <= y_max]
        if not region:
            return "", ""
        
        # 4. En bÃ¼yÃ¼k fontu bul ve tolerans bandÄ± seÃ§
        max_size = max(s["size"] for s in region)
        if max_size <= 0:
            return "", ""
        
        band = [s for s in region if s["size"] >= max_size - 4.0]
        if not band:
            return "", ""
        
        # 5. Span'leri Y pozisyonuna gÃ¶re sÄ±rala
        band.sort(key=lambda d: (d["y"], d["x"]))
        
        # 6. GÃ¼rÃ¼ltÃ¼yÃ¼ filtrele
        band = self._filter_noise_spans(band, page_h)
        if not band:
            return "", ""
        
        # 7. Span'leri satÄ±rlara grupla
        lines = self._group_spans_into_lines(band, y_tolerance=3.0)
        if not lines:
            return "", ""
        
        # 8. BaÅŸlÄ±k olmayan satÄ±rlarÄ± filtrele
        lines = self._filter_non_title_lines(lines)
        if not lines:
            return "", ""
        
        # 9. Y pozisyonuna gÃ¶re sÄ±rala
        lines.sort(key=lambda d: d["y"])
        texts = [line["text"] for line in lines]
        ys = [line["y"] for line in lines]
        
        # 10. ÃœÃ§ aÅŸamalÄ± ayÄ±rma stratejisi
        
        # Strateji 1: Gap ile ayÄ±r
        title_tr, title_en = self._split_tr_en_by_gap(texts, ys, gap_threshold=8.0)
        if title_tr and title_en:
            return title_tr, title_en
        
        # Strateji 2: Ä°ngilizce ipuÃ§larÄ±na gÃ¶re ayÄ±r
        title_tr, title_en = self._split_tr_en_by_english_hint(texts)
        if title_tr and title_en:
            return title_tr, title_en
        
        # Strateji 3: TÃ¼rkÃ§e karakter varlÄ±ÄŸÄ±na gÃ¶re ayÄ±r
        return self._split_tr_en_by_char(texts)


# ====================================================================
# PDF PROCESSOR
# ====================================================================

class PDFProcessor:
    """PDF iÅŸleme ve makale Ã§Ä±karma ana sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.page_analyzer = PageAnalyzer()
        self.title_extractor = TitleExtractor()
        self.abstract_extractor = AbstractExtractor()
    
    def process_pdf(self, pdf_path: str, year: str, output_csv: Optional[str] = None) -> List[Article]:
        """
        Tek bir PDF dosyasÄ±ndan tÃ¼m makaleleri Ã§Ä±karÄ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            year: YÄ±l bilgisi
            output_csv: Ã‡Ä±ktÄ± CSV dosya yolu (None ise otomatik oluÅŸturulur)
            
        Returns:
            Ã‡Ä±karÄ±lan Article nesnelerinin listesi
        """
        print(f"ğŸ“„ PDF aÃ§Ä±lÄ±yor: {pdf_path}")
        doc = fitz.open(pdf_path)
        print(f"ğŸ“Š Toplam sayfa sayÄ±sÄ±: {len(doc)}")
        
        articles = []
        
        # Her sayfayÄ± tara
        for page_idx in range(len(doc)):
            page = doc[page_idx]
            text = page.get_text()
            
            # Bu sayfa yeni makale baÅŸlangÄ±cÄ± mÄ±?
            if not self.page_analyzer.is_article_start_page(text):
                continue
            
            # BaÅŸlÄ±klarÄ± Ã§Ä±kar
            title_tr, title_en = self.title_extractor.extract(page)
            
            # Ã–zetleri ve anahtar kelimeleri Ã§Ä±kar
            abs_tr, abs_en, keywords_tr, keywords_en = self.abstract_extractor.extract_with_fallback(
                doc, page_idx
            )
            
            # Makale nesnesi oluÅŸtur
            article = Article(
                page_number=page_idx + 1,
                year=year,
                title_tr=title_tr,
                title_en=title_en,
                abstract_tr=abs_tr,
                abstract_en=abs_en,
                keywords_tr=keywords_tr,
                keywords_en=keywords_en
            )
            articles.append(article)
            
            # Ä°lerleme gÃ¶ster
            print(f"âœ… Sayfa {page_idx+1}: TR='{title_tr[:60]}...' | EN='{title_en[:60]}...'")
        
        doc.close()
        
        # CSV'ye yaz
        if output_csv is None:
            base = os.path.splitext(pdf_path)[0]
            output_csv = base + ".csv"
        
        self._write_to_csv(articles, output_csv)
        print(f"\nâœ¨ {len(articles)} makale bulundu. CSV yazÄ±ldÄ±: {output_csv}")
        
        return articles
    
    def _write_to_csv(self, articles: List[Article], output_path: str):
        """Makaleleri CSV dosyasÄ±na yazar"""
        fieldnames = ["PageNumber", "Year", "Title_TR", "Title_EN", 
                     "Abstract_TR", "Abstract_EN", "Keywords_TR", "Keywords_EN"]
        
        with open(output_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for article in articles:
                writer.writerow(article.to_dict())
    
    def process_path(self, input_path: str, year: str, out_dir: Optional[str] = None):
        """
        PDF dosyasÄ±, klasÃ¶r veya glob pattern'i iÅŸler.
        
        Args:
            input_path: PDF dosyasÄ±, klasÃ¶r yolu veya glob pattern
            year: YÄ±l bilgisi
            out_dir: Ã‡Ä±ktÄ± dizini (None ise PDF ile aynÄ± yerde oluÅŸturulur)
        """
        pdfs = []
        
        # KlasÃ¶r mÃ¼, dosya mÄ±, glob pattern mi?
        if os.path.isdir(input_path):
            pdfs = sorted(glob.glob(os.path.join(input_path, "*.pdf")))
        else:
            matches = glob.glob(input_path)
            if matches:
                pdfs = sorted([p for p in matches if p.lower().endswith(".pdf")])
            elif input_path.lower().endswith(".pdf"):
                pdfs = [input_path]
        
        if not pdfs:
            raise FileNotFoundError(f"âŒ PDF bulunamadÄ±: {input_path}")
        
        print(f"\nğŸ” {len(pdfs)} PDF dosyasÄ± bulundu\n")
        
        # Ã‡Ä±ktÄ± dizinini oluÅŸtur
        if out_dir is not None:
            os.makedirs(out_dir, exist_ok=True)
        
        # Her PDF'i iÅŸle
        for idx, pdf in enumerate(pdfs, 1):
            print(f"\n{'='*80}")
            print(f"[{idx}/{len(pdfs)}] Ä°ÅŸleniyor...")
            print(f"{'='*80}")
            
            if out_dir:
                out_csv = os.path.join(out_dir, os.path.splitext(os.path.basename(pdf))[0] + ".csv")
            else:
                out_csv = None
            
            self.process_pdf(pdf, year, out_csv)


# ====================================================================
# MAIN EXECUTION
# ====================================================================

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    # KonfigÃ¼rasyon
    YEAR = "2021-2022"
    PDF_PATH = f"Bildiri-Kitabi-{YEAR}.pdf"
    OUTPUT_DIR = None
    
    print("="*80)
    print("LIFT UP Dataset Extraction Tool (OOP Implementation)")
    print("="*80)
    print(f"PDF Path: {PDF_PATH}")
    print(f"Year: {YEAR}")
    print(f"Output Dir: {OUTPUT_DIR}")
    print("="*80 + "\n")
    
    try:
        processor = PDFProcessor()
        processor.process_path(PDF_PATH, YEAR, OUTPUT_DIR)
        print("\nğŸ‰ Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())
